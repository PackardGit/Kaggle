{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle Titanic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGCQc4KmCmkv"
      },
      "source": [
        "# Kaggle Competition for Titanic Data #\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enlT-PyGwul9",
        "outputId": "8395e5d5-ce1e-4336-8262-ee72fc92fb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "# Mount Google Drive and define time function #\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "COLAB = True\n",
        "print(\"Note: using Google CoLab\")\n",
        "\n",
        "import time \n",
        "\n",
        "def time_elapsed(sec):\n",
        "  h = int(sec/3600)\n",
        "  m = int(sec/60)\n",
        "  s = sec % 60\n",
        "  return \"{}:{>02}:{:>05.2f}\".format(h,m,s)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoyr9RKwIacX"
      },
      "source": [
        "def prepare_df(df):\n",
        "  # Create a Title Column #\n",
        "  df.insert(2, \"Title\",'') \n",
        "\n",
        "  df.loc[df.Name.str.contains(pat = 'Mr'),'Title']='Mr' \n",
        "  df.loc[df.Name.str.contains(pat = 'Master'),'Title']='Master' \n",
        "  df.loc[df.Name.str.contains(pat = 'Miss'),'Title']='Miss' \n",
        "  df.loc[df.Name.str.contains(pat = 'Mrs'),'Title']='Mrs' \n",
        "\n",
        "  # filling age gaps #\n",
        "  # Age depends on title and parch #\n",
        "\n",
        "  # get median age of female childreen # Title Miss means unmarried woman and Parch > 0 means traveling with parents/childreen\n",
        "  female_Miss_childreen = df[(df.Title=='Miss') & (df.Parch > 0) ][\"Age\"].median()\n",
        "  female_Miss_adult = df[(df.Title=='Miss') & (df.Parch == 0) ][\"Age\"].median() # female adult without husband\n",
        "  female_Mrs_adult = df[(df.Title=='Mrs')][\"Age\"].median() # female adult with husband\n",
        "  male_Master = df[(df.Title=='Master')][\"Age\"].median() # male child\n",
        "  male_Mr = df[(df.Title=='Mr')][\"Age\"].median()     \n",
        "  overal_median = df[\"Age\"].median()\n",
        "\n",
        "  # fill those empty age gaps\n",
        "  df.loc[(df.Title=='Miss') & (df.Parch > 0) & (df.Age.isnull()), \"Age\"] = female_Miss_childreen\n",
        "  df.loc[(df.Title=='Miss') & (df.Parch == 0) & (df.Age.isnull()), \"Age\"] = female_Miss_adult\n",
        "  df.loc[(df.Title=='Mrs') & (df.Age.isnull()), \"Age\"] = female_Mrs_adult\n",
        "  df.loc[(df.Title=='Mr') & (df.Age.isnull()), \"Age\"] = male_Mr\n",
        "  df.loc[(df.Title=='Master') & (df.Age.isnull()), \"Age\"] = male_Master\n",
        "  df.loc[df.Age.isnull(), \"Age\"] =  overal_median\n",
        "\n",
        "  # Age is not much important factor while you are above 18, so lets make column to separate kids from adults #\n",
        "  # Column named \"Adult\" 1 means age is above 18, 0-belov\n",
        "  df.insert(2, \"Adult\",'') \n",
        "  df.loc[(df.Age > 18), \"Adult\"] = 1.0\n",
        "  df.loc[(df.Age <= 18), \"Adult\"] = 0.0\n",
        "  df.loc[(df.Age.isnull()), \"Adult\"] = 1\n",
        "  df[\"Adult\"] = pd.to_numeric(df.Adult, errors='coerce') # to change from type obcjet to float\n",
        "\n",
        "  # Important factor is to survive is Fare, more expensive ticket more chance to survive\n",
        "  # but some people have the same ticket so need to divide their Fare to numbers of people sharing their ticket\n",
        "  # then need to fill missing Fare gaps acording to Pclass they belong\n",
        "\n",
        "  # Adding column Fare per Person #\n",
        "  df.insert(12, \"FarePP\",0) \n",
        "\n",
        "  # Evaluating Fare PP #\n",
        "  tickets_count = df.groupby(\"Ticket\")[\"Ticket\"].count() #counting tickets\n",
        "  for ticket_number in tickets_count.keys():\n",
        "    fpp = (df[df.Ticket == ticket_number][\"Fare\"]) / tickets_count[ticket_number]\n",
        "    df.loc[df.Ticket == ticket_number, \"FarePP\"] = fpp\n",
        "\n",
        "  # filling gaps\n",
        "  FarePP_1_class = df[(df.Pclass==1) ][\"FarePP\"].median()\n",
        "  FarePP_2_class = df[(df.Pclass==2) ][\"FarePP\"].median()\n",
        "  FarePP_3_class = df[(df.Pclass==3) ][\"FarePP\"].median()\n",
        "\n",
        "  df.loc[(df.Pclass==1) & (df.FarePP.isnull()), \"FarePP\"] = FarePP_1_class\n",
        "  df.loc[(df.Pclass==2) & (df.FarePP.isnull()), \"FarePP\"] = FarePP_2_class\n",
        "  df.loc[(df.Pclass==3) & (df.FarePP.isnull()), \"FarePP\"] = FarePP_3_class\n",
        "\n",
        "  df = pd.concat([df,pd.get_dummies(df['Sex'],prefix=\"Sex\")],axis=1)\n",
        "  df.drop('Sex', axis=1, inplace=True)\n",
        "  df = pd.concat([df,pd.get_dummies(df['Embarked'],prefix=\"Embarked\")],axis=1)\n",
        "  df.drop('Embarked', axis=1, inplace=True)\n",
        "\n",
        "  df = pd.concat([df,pd.get_dummies(df['Title'],prefix=\"Title\")],axis=1)\n",
        "  df.drop('Title', axis=1, inplace=True)\n",
        "\n",
        "  df['FarePP'] = zscore(df['FarePP'])\n",
        "  df['Age'] = zscore(df['Age'])\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-O6HYUCctH"
      },
      "source": [
        "# Read Titanic Data #\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "\n",
        "DATA_PATH = '/content/drive/My Drive/projects/titanic'\n",
        "TRAIN_FILE_PATH = os.path.join(DATA_PATH, 'train.csv') \n",
        "TEST_FILE_PATH = os.path.join(DATA_PATH, 'test.csv') \n",
        "\n",
        "\n",
        "df = pd.read_csv(TRAIN_FILE_PATH,na_values=['NA','?'])\n",
        "df = prepare_df(df)\n",
        "\n",
        "y = df['Survived'].values\n",
        "x_columns = df.columns.drop('PassengerId').drop('Survived').drop('Ticket').drop('Fare').drop('Cabin').drop('Name')\n",
        "x = df[x_columns].values\n",
        "\n",
        "df_s = pd.read_csv(TEST_FILE_PATH,na_values=['NA','?'])\n",
        "df_s = prepare_df(df_s)\n",
        "x_columns_s = df_s.columns.drop('PassengerId').drop('Ticket').drop('Fare').drop('Cabin').drop('Name')\n",
        "\n",
        "x_submit = df_s[x_columns_s].values\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ig7oEB7glp_"
      },
      "source": [
        "# stworzenie własnej sieci neuronowej\n",
        "import tensorflow.keras.initializers\n",
        "import statistics\n",
        "import tensorflow.keras\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import math\n",
        "\n",
        "def mlogloss(y_test, preds): # funckja \n",
        "    epsilon = 1e-15\n",
        "    sum = 0\n",
        "    for row in zip(preds,y_test):\n",
        "        x = row[0][row[1]]\n",
        "        x = max(epsilon,x)\n",
        "        x = min(1-epsilon,x)\n",
        "        sum+=math.log(x)\n",
        "    return( (-1/len(preds))*sum)\n",
        "\n",
        "def build_snn(input_size):\n",
        "    neuronPct = 0.9652  # parametry zooptymalizowane za pomoca BayesianOplimization\n",
        "    neuronShrink = 0.7319\n",
        "    dropout = 0.01058\n",
        "    lr = 0.08079\n",
        "    neuronCount = int(neuronPct * 1000)   \n",
        "    model = Sequential()\n",
        "    layer = 0\n",
        "    while neuronCount>25 and layer<4:\n",
        "        # The first (0th) layer needs an input input_dim(neuronCount)\n",
        "        if layer==0:\n",
        "            model.add(Dense(neuronCount,input_dim=input_size,activation=PReLU()))\n",
        "        else:\n",
        "            model.add(Dense(neuronCount, activation=PReLU())) \n",
        "        layer += 1\n",
        "        # Add dropout after each hidden layer\n",
        "        model.add(Dropout(dropout))\n",
        "        # Shrink neuron count for each layer\n",
        "        neuronCount = neuronCount * neuronShrink\n",
        "    model.add(Dense(1,activation='sigmoid')) # Output\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr))\n",
        "    return model\n",
        "\n",
        "def blend_ensamble(x,y,x_submit):\n",
        "  FOLDS = 5 # podzielenie danych treningowych na 10 czesci\n",
        "  kf = StratifiedKFold(FOLDS,shuffle=True, random_state=0)\n",
        "  folds = list(kf.split(x,y))\n",
        "\n",
        "  models = [\n",
        "            #SVC(kernel='rbf',shrinking=True,probability=True),\n",
        "            KerasClassifier(build_fn = build_snn, input_size=x.shape[1]),\n",
        "            KNeighborsClassifier(n_neighbors=5),\n",
        "            RandomForestClassifier(n_estimators=200,n_jobs=-1,criterion='gini'),\n",
        "            RandomForestClassifier(n_estimators=200,n_jobs=-1,criterion='entropy'),\n",
        "            ExtraTreesClassifier(n_estimators=200,n_jobs=-1,criterion='gini'),\n",
        "            ExtraTreesClassifier(n_estimators=200,n_jobs=-1,criterion='entropy'),\n",
        "            GradientBoostingClassifier(learning_rate=0.05,subsample=0.5,max_depth=6,n_estimators=50)]\n",
        "\n",
        "  dataset_blend_train = np.zeros((x.shape[0], len(models))) # każdy model wypracuje jedna wartosc dla jednej danej\n",
        "  dataset_blend_test = np.zeros((x_submit.shape[0], len(models))) \n",
        "\n",
        "  for j, model in enumerate(models): # dla każdego modelu\n",
        "    print(f'{j} Model: {model}')\n",
        "    fold_sums = np.zeros((x_submit.shape[0],len(folds))) # dla każdego zestawu danych wyznaczone beda predyckje i wysrednione\n",
        "    total_loss = 0\n",
        "    for i, (train,test) in enumerate(folds):  # dla każdego zestawu danych treningowych\n",
        "      x_train = x[train]\n",
        "      y_train = y[train]\n",
        "      x_test = x[test] # x testowe ale ze zbioru treningowego\n",
        "      y_test = y[test]\n",
        "      if j == 0:\n",
        "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "        patience=10, verbose=0, mode='auto', restore_best_weights=True)\n",
        "\n",
        "        # Train on the bootstrap sample\n",
        "        model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "                  callbacks=[monitor],verbose=0,epochs=300)\n",
        "      else:\n",
        "        model.fit(x_train,y_train)\n",
        "      pred = np.array(model.predict_proba(x_test))\n",
        "      pred_submit = np.array(model.predict_proba(x_submit))\n",
        "      dataset_blend_train[test,j] = pred[:,1] # zapisanie predykcji dla danego modelu oraz danego zestawu danych\n",
        "      fold_sums[:,i] = pred_submit[:,1] # dla kazdego zestawu danych policzyc predykcje\n",
        "      loss = mlogloss(y_test,pred) # policzenie pomocniczej funkcji celu\n",
        "      total_loss+=loss # liczenie sumy funkcji celu dla modelu\n",
        "      print(f'Fold {i} has loss: {loss}') \n",
        "    print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
        "    dataset_blend_test[:,j] = fold_sums.mean(1) # wyznaczenie jednej predykcji dla jednego modelu\n",
        "  \n",
        "  print('Blending Models')\n",
        "  # wyznaczenie ostatecznej predykcji za pomoca regresji logistycznej\n",
        "# ostateczne predykcja na podstawie predykcji wszystkich modeli\n",
        "  return dataset_blend_train,dataset_blend_test\n",
        "def stretch(y):\n",
        "    return (y - y.min()) / (y.max() - y.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "067UEBPhjW43",
        "outputId": "127b3bae-caf6-4cc7-ac26-6b2251a5d928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "(dataset_blend_train,dataset_blend_test) = blend_ensamble(x,y,x_submit)\n",
        "#submit_data = stretch(submit_data)\n",
        "\n",
        "blend = LogisticRegression(solver='lbfgs') \n",
        "blend.fit(dataset_blend_train,y) \n",
        "wynik=blend.predict_proba(dataset_blend_test) \n",
        "\n",
        "submit_data = stretch(wynik)\n",
        "submit_data = np.round( submit_data[:, 1])\n",
        "submit_data = submit_data.astype(int)\n",
        "ids = [id+892 for id in range(len(submit_data))]\n",
        "submit_filename = os.path.join(DATA_PATH, \"Titanic_submit.csv\")\n",
        "submit_df = pd.DataFrame({'PassengerId': ids, \n",
        "                              'Survived': \n",
        "                              submit_data},\n",
        "                             columns=['PassengerId',\n",
        "                            'Survived'])\n",
        "submit_df.to_csv(submit_filename, index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Model: <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fd012241470>\n",
            "Fold 0 has loss: 0.6949571494494441\n",
            "Fold 1 has loss: 0.5790863346383671\n",
            "Fold 2 has loss: 0.8609926147730738\n",
            "Fold 3 has loss: 0.6053200842146472\n",
            "Fold 4 has loss: 0.7338222898233905\n",
            "KerasClassifier: Mean loss=0.6948356945797846\n",
            "1 Model: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform')\n",
            "Fold 0 has loss: 1.6059721511101148\n",
            "Fold 1 has loss: 1.1193560597653673\n",
            "Fold 2 has loss: 1.65715687015957\n",
            "Fold 3 has loss: 2.6461994230460353\n",
            "Fold 4 has loss: 2.78477664181618\n",
            "KNeighborsClassifier: Mean loss=1.9626922291794535\n",
            "2 Model: RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
            "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
            "                       warm_start=False)\n",
            "Fold 0 has loss: 0.8981995141284282\n",
            "Fold 1 has loss: 0.43064226976699505\n",
            "Fold 2 has loss: 0.8060391684148865\n",
            "Fold 3 has loss: 0.6855717135123759\n",
            "Fold 4 has loss: 1.0386580160142216\n",
            "RandomForestClassifier: Mean loss=0.7718221363673814\n",
            "3 Model: RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='entropy', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
            "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
            "                       warm_start=False)\n",
            "Fold 0 has loss: 0.8944782537835542\n",
            "Fold 1 has loss: 0.4413862938655479\n",
            "Fold 2 has loss: 0.64666173773187\n",
            "Fold 3 has loss: 0.5411470071636171\n",
            "Fold 4 has loss: 1.2090335356900086\n",
            "RandomForestClassifier: Mean loss=0.7465413656469195\n",
            "4 Model: ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
            "                     criterion='gini', max_depth=None, max_features='auto',\n",
            "                     max_leaf_nodes=None, max_samples=None,\n",
            "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                     min_samples_leaf=1, min_samples_split=2,\n",
            "                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
            "                     oob_score=False, random_state=None, verbose=0,\n",
            "                     warm_start=False)\n",
            "Fold 0 has loss: 1.4742770730597157\n",
            "Fold 1 has loss: 1.181688947654089\n",
            "Fold 2 has loss: 2.0798271374663573\n",
            "Fold 3 has loss: 1.4465919504210156\n",
            "Fold 4 has loss: 2.6827147553843202\n",
            "ExtraTreesClassifier: Mean loss=1.7730199727970994\n",
            "5 Model: ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
            "                     criterion='entropy', max_depth=None, max_features='auto',\n",
            "                     max_leaf_nodes=None, max_samples=None,\n",
            "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                     min_samples_leaf=1, min_samples_split=2,\n",
            "                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
            "                     oob_score=False, random_state=None, verbose=0,\n",
            "                     warm_start=False)\n",
            "Fold 0 has loss: 1.4825249441914832\n",
            "Fold 1 has loss: 1.1792832065209284\n",
            "Fold 2 has loss: 2.0845202884236342\n",
            "Fold 3 has loss: 1.4440921103476625\n",
            "Fold 4 has loss: 2.84035112846101\n",
            "ExtraTreesClassifier: Mean loss=1.8061543355889433\n",
            "6 Model: GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
            "                           learning_rate=0.05, loss='deviance', max_depth=6,\n",
            "                           max_features=None, max_leaf_nodes=None,\n",
            "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                           min_samples_leaf=1, min_samples_split=2,\n",
            "                           min_weight_fraction_leaf=0.0, n_estimators=50,\n",
            "                           n_iter_no_change=None, presort='deprecated',\n",
            "                           random_state=None, subsample=0.5, tol=0.0001,\n",
            "                           validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False)\n",
            "Fold 0 has loss: 0.3736876428561972\n",
            "Fold 1 has loss: 0.3887947022667705\n",
            "Fold 2 has loss: 0.4085782875056381\n",
            "Fold 3 has loss: 0.44446255734915596\n",
            "Fold 4 has loss: 0.46973711125515066\n",
            "GradientBoostingClassifier: Mean loss=0.4170520602465825\n",
            "Blending Models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-oPHMvZqD4O"
      },
      "source": [
        "ids = [id+892 for id in range(len(submit_data))]\n",
        "submit_filename = os.path.join(DATA_PATH, \"Titanic_submit.csv\")\n",
        "submit_df = pd.DataFrame({'PassengerId': ids, \n",
        "                              'Survived': \n",
        "                              submit_data},\n",
        "                             columns=['PassengerId',\n",
        "                            'Survived'])\n",
        "submit_df.to_csv(submit_filename, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Rku1BqrMXt",
        "outputId": "6f74834b-f4f1-4f3a-8004-f344d062a7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "y_submition-submit_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    }
  ]
}