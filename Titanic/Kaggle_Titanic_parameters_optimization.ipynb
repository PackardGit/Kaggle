{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic parameters optimization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3WOvlJGnnX8",
        "outputId": "f61980bb-914c-45c2-fb53-084ab6c0811b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Mount Google Drive and define time function #\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "COLAB = True\n",
        "print(\"Note: using Google CoLab\")\n",
        "\n",
        "import time \n",
        "\n",
        "def time_elapsed(sec):\n",
        "  h = int(sec/3600)\n",
        "  m = int(sec/60)\n",
        "  s = sec % 60\n",
        "  return \"{}:{>02}:{:>05.2f}\".format(h,m,s)\n",
        "\n",
        "# Read Titanic Data #\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "\n",
        "DATA_PATH = '/content/drive/My Drive/projects/titanic'\n",
        "TRAIN_FILE_PATH = os.path.join(DATA_PATH, 'train.csv') \n",
        "TEST_FILE_PATH = os.path.join(DATA_PATH, 'test.csv') \n",
        "\n",
        "df = pd.read_csv(TRAIN_FILE_PATH,na_values=['NA','?'])\n",
        "df_test = pd.read_csv(TEST_FILE_PATH,na_values=['NA','?'])\n",
        "\n",
        "# Create a Title Column #\n",
        "df.insert(2, \"Title\",'') \n",
        "\n",
        "df.loc[df.Name.str.contains(pat = 'Mr'),'Title']='Mr' \n",
        "df.loc[df.Name.str.contains(pat = 'Master'),'Title']='Master' \n",
        "df.loc[df.Name.str.contains(pat = 'Miss'),'Title']='Miss' \n",
        "df.loc[df.Name.str.contains(pat = 'Mrs'),'Title']='Mrs' \n",
        "\n",
        "# filling age gaps #\n",
        "# Age depends on title and parch #\n",
        "\n",
        "# get median age of female childreen # Title Miss means unmarried woman and Parch > 0 means traveling with parents/childreen\n",
        "female_Miss_childreen = df[(df.Title=='Miss') & (df.Parch > 0) ][\"Age\"].median()\n",
        "female_Miss_adult = df[(df.Title=='Miss') & (df.Parch == 0) ][\"Age\"].median() # female adult without husband\n",
        "female_Mrs_adult = df[(df.Title=='Mrs')][\"Age\"].median() # female adult with husband\n",
        "male_Master = df[(df.Title=='Master')][\"Age\"].median() # male child\n",
        "male_Mr = df[(df.Title=='Mr')][\"Age\"].median()                 \n",
        "# fill those empty age gaps\n",
        "df.loc[(df.Title=='Miss') & (df.Parch > 0) & (df.Age.isnull()), \"Age\"] = female_Miss_childreen\n",
        "df.loc[(df.Title=='Miss') & (df.Parch == 0) & (df.Age.isnull()), \"Age\"] = female_Miss_adult\n",
        "df.loc[(df.Title=='Mrs') & (df.Age.isnull()), \"Age\"] = female_Mrs_adult\n",
        "df.loc[(df.Title=='Mr') & (df.Age.isnull()), \"Age\"] = male_Mr\n",
        "df.loc[(df.Title=='Master') & (df.Age.isnull()), \"Age\"] = male_Master\n",
        "df.loc[(df.Age.isnull()), \"Age\"] = male_Master\n",
        "# Age is not much important factor while you are above 18, so lets make column to separate kids from adults #\n",
        "# Column named \"Adult\" 1 means age is above 18, 0-belov\n",
        "df.insert(2, \"Adult\",'') \n",
        "df.loc[(df.Age > 18), \"Adult\"] = 1\n",
        "df.loc[(df.Age <= 18), \"Adult\"] = 0\n",
        "df.loc[(df.Age.isnull()), \"Adult\"] = 1\n",
        "df[\"Adult\"] = pd.to_numeric(df.Adult, errors='coerce')\n",
        "\n",
        "# Important factor is to survive is Fare, more expensive ticket more chance to survive\n",
        "# but some people have the same ticket so need to divide their Fare to numbers of people sharing their ticket\n",
        "# then need to fill missing Fare gaps acording to Pclass they belong\n",
        "\n",
        "# Adding column Fare per Person #\n",
        "df.insert(12, \"FarePP\",0) \n",
        "\n",
        "# Evaluating Fare PP #\n",
        "tickets_count = df.groupby(\"Ticket\")[\"Ticket\"].count() #counting tickets\n",
        "for ticket_number in tickets_count.keys():\n",
        "  fpp = (df[df.Ticket == ticket_number][\"Fare\"]) / tickets_count[ticket_number]\n",
        "  df.loc[df.Ticket == ticket_number, \"FarePP\"] = fpp\n",
        "\n",
        "# filling gaps\n",
        "FarePP_1_class = df[(df.Pclass==1) ][\"FarePP\"].median()\n",
        "FarePP_2_class = df[(df.Pclass==2) ][\"FarePP\"].median()\n",
        "FarePP_3_class = df[(df.Pclass==3) ][\"FarePP\"].median()\n",
        "\n",
        "df.loc[(df.Pclass==1) & (df.FarePP.isnull()), \"FarePP\"] = FarePP_1_class\n",
        "df.loc[(df.Pclass==2) & (df.FarePP.isnull()), \"FarePP\"] = FarePP_2_class\n",
        "df.loc[(df.Pclass==3) & (df.FarePP.isnull()), \"FarePP\"] = FarePP_3_class\n",
        "\n",
        "df = pd.concat([df,pd.get_dummies(df['Sex'],prefix=\"Sex\")],axis=1)\n",
        "df.drop('Sex', axis=1, inplace=True)\n",
        "df = pd.concat([df,pd.get_dummies(df['Embarked'],prefix=\"Embarked\")],axis=1)\n",
        "df.drop('Embarked', axis=1, inplace=True)\n",
        "df['FarePP'] = zscore(df['FarePP'])\n",
        "df['Age'] = zscore(df['Age'])\n",
        "# decide which columns are to drop\n",
        "y = df['Survived'].values\n",
        "x_columns = df.columns.drop('PassengerId').drop('Survived').drop('Title').drop('Ticket').drop('Fare').drop('Cabin').drop('Name').drop('Age')\n",
        "x = df[x_columns].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZUQSRHqn_Y2"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow.keras.initializers\n",
        "import statistics\n",
        "import tensorflow.keras\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def generate_model(dropout, neuronPct, neuronShrink):\n",
        "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
        "    neuronCount = int(neuronPct * 1000)\n",
        "    \n",
        "    # Construct neural network\n",
        "    # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n",
        "    model = Sequential()\n",
        "\n",
        "    # So long as there would have been at least 25 neurons and fewer than 10\n",
        "    # layers, create a new layer.\n",
        "    layer = 0\n",
        "    while neuronCount>25 and layer<4:\n",
        "        # The first (0th) layer needs an input input_dim(neuronCount)\n",
        "        if layer==0:\n",
        "            model.add(Dense(neuronCount, \n",
        "                input_dim=x.shape[1], \n",
        "                activation=PReLU()))\n",
        "        else:\n",
        "            model.add(Dense(neuronCount, activation=PReLU())) \n",
        "        layer += 1\n",
        "\n",
        "        # Add dropout after each hidden layer\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        # Shrink neuron count for each layer\n",
        "        neuronCount = neuronCount * neuronShrink\n",
        "\n",
        "    model.add(Dense(1,activation='sigmoid')) # Output\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqRZQwXEoBzO"
      },
      "source": [
        "  \n",
        "def evaluate_network(dropout,lr,neuronPct,neuronShrink):\n",
        "    SPLITS = 2\n",
        "\n",
        "    # Bootstrap\n",
        "    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
        "\n",
        "    # Track progress\n",
        "    mean_benchmark = []\n",
        "    epochs_needed = []\n",
        "    num = 0\n",
        "    \n",
        "\n",
        "    # Loop through samples\n",
        "    for train, test in boot.split(x,y):\n",
        "        start_time = time.time()\n",
        "        num+=1\n",
        "\n",
        "        # Split train and test\n",
        "        x_train = x[train]\n",
        "        y_train = y[train]\n",
        "        x_test = x[test]\n",
        "        y_test = y[test]\n",
        "        y_test\n",
        "        model = generate_model(dropout, neuronPct, neuronShrink)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr))\n",
        "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "        patience=50, verbose=0, mode='auto', restore_best_weights=True)\n",
        "\n",
        "        # Train on the bootstrap sample\n",
        "        model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "                  callbacks=[monitor],verbose=0,epochs=200)\n",
        "        epochs = monitor.stopped_epoch\n",
        "        epochs_needed.append(epochs)\n",
        "\n",
        "        # Predict on the out of boot (validation)\n",
        "        pred = model.predict(x_test)\n",
        "\n",
        "         #Measure this bootstrap's log loss\n",
        "        y_compare = y_test # For log loss calculation\n",
        "\n",
        "        y_compare = np.array(y_test)\n",
        "        y_compare = y_compare.astype('float32')\n",
        "        y_compare = np.round(y_compare)\n",
        "        score_temp = np.zeros(len(y_test))\n",
        "        for i in range(len(y_test)):\n",
        "          score_temp[i] = math.sqrt((y_compare[i]-pred[i])*(y_compare[i]-pred[i]))\n",
        "        score = sum(score_temp)\n",
        "        mean_benchmark.append(score)\n",
        "        m1 = statistics.mean(mean_benchmark)\n",
        "       \n",
        "        m2 = statistics.mean(epochs_needed)\n",
        "        mdev = statistics.pstdev(mean_benchmark)\n",
        "\n",
        "        # Record this iteration\n",
        "        time_took = time.time() - start_time\n",
        "        \n",
        "    tensorflow.keras.backend.clear_session()\n",
        "    return (-m1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2-dMjb4oD0n",
        "outputId": "165fa5e3-f7a2-4ab2-bcfc-2b1130d08d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "import time\n",
        "\n",
        "# Supress NaN warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'dropout': (0.0, 0.499),\n",
        "           'lr': (0.000001, 0.1),\n",
        "           'neuronPct': (0.01, 1),\n",
        "           'neuronShrink': (0.1, 1)\n",
        "          }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_network,\n",
        "    pbounds=pbounds,\n",
        "    verbose=2,  # verbose = 1 prints only when a maximum \n",
        "    # is observed, verbose = 0 is silent\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer.maximize(init_points=4, n_iter=20,)\n",
        "time_took = time.time() - start_time\n",
        "\n",
        "print(f\"Total runtime: {hms_string(time_took)}\")\n",
        "print(optimizer.max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |  dropout  |    lr     | neuronPct | neuron... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-25.14   \u001b[0m | \u001b[0m 0.2081  \u001b[0m | \u001b[0m 0.07203 \u001b[0m | \u001b[0m 0.01011 \u001b[0m | \u001b[0m 0.3721  \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-22.5    \u001b[0m | \u001b[95m 0.07323 \u001b[0m | \u001b[95m 0.009235\u001b[0m | \u001b[95m 0.1944  \u001b[0m | \u001b[95m 0.411   \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m-20.04   \u001b[0m | \u001b[95m 0.198   \u001b[0m | \u001b[95m 0.05388 \u001b[0m | \u001b[95m 0.425   \u001b[0m | \u001b[95m 0.7167  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-27.43   \u001b[0m | \u001b[0m 0.102   \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.03711 \u001b[0m | \u001b[0m 0.7034  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-23.44   \u001b[0m | \u001b[0m 0.003852\u001b[0m | \u001b[0m 0.01619 \u001b[0m | \u001b[0m 0.7123  \u001b[0m | \u001b[0m 0.7883  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-24.43   \u001b[0m | \u001b[0m 0.2208  \u001b[0m | \u001b[0m 0.04135 \u001b[0m | \u001b[0m 0.5523  \u001b[0m | \u001b[0m 0.7698  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-29.58   \u001b[0m | \u001b[0m 0.3111  \u001b[0m | \u001b[0m 0.02232 \u001b[0m | \u001b[0m 0.9595  \u001b[0m | \u001b[0m 0.8242  \u001b[0m |\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m-17.72   \u001b[0m | \u001b[95m 0.01058 \u001b[0m | \u001b[95m 0.08079 \u001b[0m | \u001b[95m 0.9652  \u001b[0m | \u001b[95m 0.7319  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-28.67   \u001b[0m | \u001b[0m 0.1818  \u001b[0m | \u001b[0m 0.002241\u001b[0m | \u001b[0m 0.1609  \u001b[0m | \u001b[0m 0.9744  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-24.44   \u001b[0m | \u001b[0m 0.4967  \u001b[0m | \u001b[0m 0.03252 \u001b[0m | \u001b[0m 0.8187  \u001b[0m | \u001b[0m 0.1402  \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-30.08   \u001b[0m | \u001b[0m 0.4896  \u001b[0m | \u001b[0m 0.02704 \u001b[0m | \u001b[0m 0.8107  \u001b[0m | \u001b[0m 0.5019  \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-25.51   \u001b[0m | \u001b[0m 0.1621  \u001b[0m | \u001b[0m 0.06358 \u001b[0m | \u001b[0m 0.4065  \u001b[0m | \u001b[0m 0.7764  \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-25.86   \u001b[0m | \u001b[0m 0.1526  \u001b[0m | \u001b[0m 0.06767 \u001b[0m | \u001b[0m 0.5559  \u001b[0m | \u001b[0m 0.3843  \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-23.74   \u001b[0m | \u001b[0m 0.1982  \u001b[0m | \u001b[0m 0.01942 \u001b[0m | \u001b[0m 0.06233 \u001b[0m | \u001b[0m 0.4105  \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-25.27   \u001b[0m | \u001b[0m 0.004722\u001b[0m | \u001b[0m 0.05502 \u001b[0m | \u001b[0m 0.1704  \u001b[0m | \u001b[0m 0.53    \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-27.01   \u001b[0m | \u001b[0m 0.1658  \u001b[0m | \u001b[0m 0.04848 \u001b[0m | \u001b[0m 0.4227  \u001b[0m | \u001b[0m 0.8387  \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-22.6    \u001b[0m | \u001b[0m 0.1168  \u001b[0m | \u001b[0m 0.0447  \u001b[0m | \u001b[0m 0.5546  \u001b[0m | \u001b[0m 0.9543  \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-25.47   \u001b[0m | \u001b[0m 0.1965  \u001b[0m | \u001b[0m 0.05824 \u001b[0m | \u001b[0m 0.4104  \u001b[0m | \u001b[0m 0.7208  \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-21.67   \u001b[0m | \u001b[0m 0.1311  \u001b[0m | \u001b[0m 0.002184\u001b[0m | \u001b[0m 0.1758  \u001b[0m | \u001b[0m 0.6775  \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-21.83   \u001b[0m | \u001b[0m 0.09784 \u001b[0m | \u001b[0m 0.04004 \u001b[0m | \u001b[0m 0.5202  \u001b[0m | \u001b[0m 0.1631  \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-24.19   \u001b[0m | \u001b[0m 0.09021 \u001b[0m | \u001b[0m 0.02718 \u001b[0m | \u001b[0m 0.8055  \u001b[0m | \u001b[0m 0.6832  \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-24.87   \u001b[0m | \u001b[0m 0.06672 \u001b[0m | \u001b[0m 0.05079 \u001b[0m | \u001b[0m 0.8902  \u001b[0m | \u001b[0m 0.6104  \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-25.05   \u001b[0m | \u001b[0m 0.07567 \u001b[0m | \u001b[0m 0.02043 \u001b[0m | \u001b[0m 0.6758  \u001b[0m | \u001b[0m 0.8873  \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-30.12   \u001b[0m | \u001b[0m 0.4082  \u001b[0m | \u001b[0m 0.01635 \u001b[0m | \u001b[0m 0.02488 \u001b[0m | \u001b[0m 0.2449  \u001b[0m |\n",
            "=========================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-214-103b6c40cf13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtime_took\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total runtime: {hms_string(time_took)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hms_string' is not defined"
          ]
        }
      ]
    }
  ]
}